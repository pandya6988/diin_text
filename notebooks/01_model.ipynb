{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model \n",
    "\n",
    "> help library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class ForwardHook():\n",
    "    def __init__(self, module, name:None, activation:bool, stats:bool):\n",
    "        ''' Track stats and activations of any layer in model '''\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.status = 'active'\n",
    "        self.name = name\n",
    "        self.activation_status = activation\n",
    "        self.stats = stats\n",
    "        if self.activation_status: self.activations = []\n",
    "        if stats: self.means, self.stds = [], []\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if self.activation_status: self.activations.append ( output[1].detach().cpu().numpy() )\n",
    "        if self.stats: \n",
    "          self.means += output[1].mean(1).detach().cpu().numpy().squeeze().tolist() \n",
    "          self.stds += output[1].std(1).detach().cpu().numpy().squeeze().tolist()\n",
    "\n",
    "    def _stack_activations(self):\n",
    "        self.activations = np.vstack(self.activations)\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        self.status = 'removed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: <br>\n",
    "- module(nn.Module): Module example: model.linear1\n",
    "- name(str): Tag \n",
    "- activation(bool): if True, it will track module output\n",
    "- stats(bool): if True, it will track module output mean and std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_stack_activations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can be inheritet this class and define your own `hook_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_linear_layer_activations_states(dls, model, layers:list, stats:bool, activation:bool, remove_hooks=True):\n",
    "\n",
    "  hooks = dict()\n",
    "  output_dict = {'senti': list(), 'lang': list()}\n",
    "\n",
    "  for layer in layers:\n",
    "    if hasattr(model, layer): hooks[f'{layer}'] = ForwardHook ( getattr(model, layer),layer, activation=activation, stats=stats ) \n",
    "\n",
    "  model.eval()\n",
    "  i = 0\n",
    "  for batch in tqdm(dls, desc='Fatching data: '):\n",
    "    with torch.no_grad():\n",
    "        i+=1\n",
    "        pred_senti, pred_lang = model(batch['ids'], batch['mask'], batch['token_type_ids'])\n",
    "        output_dict['senti'] += pred_senti.sigmoid().round().squeeze().detach().cpu().numpy().tolist()\n",
    "        output_dict['lang'] += pred_lang.sigmoid().round().squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "  if stats: means, stds = dict(), dict()\n",
    "  if activation: activations = dict()\n",
    "\n",
    "  for k,v in hooks.items():\n",
    "    if activation: v._stack_activations()\n",
    "    if remove_hooks: v.close()\n",
    "\n",
    "  return hooks, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3712jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
