{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nbdev.showdoc import show_doc\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model \n",
    "\n",
    "> help library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class ForwardHook():\n",
    "    def __init__(self, module, name:None, activation:bool, stats:bool):\n",
    "        ''' Track stats and activations of any layer in model '''\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.status = 'active'\n",
    "        self.name = name\n",
    "        self.activation_status = activation\n",
    "        self.stats = stats\n",
    "        if self.activation_status: self.activations = []\n",
    "        if stats: self.means, self.stds = [], []\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if self.activation_status: self.activations.append ( output[1].detach().cpu().numpy() )\n",
    "        if self.stats: \n",
    "          self.means += output[1].mean(1).detach().cpu().numpy().squeeze().tolist() \n",
    "          self.stds += output[1].std(1).detach().cpu().numpy().squeeze().tolist()\n",
    "\n",
    "    def _stack_activations(self):\n",
    "        self.activations = np.vstack(self.activations)\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        self.status = 'removed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: <br>\n",
    "- module(nn.Module): Module example: model.linear1\n",
    "- name(str): Tag \n",
    "- activation(bool): if True, it will track module output\n",
    "- stats(bool): if True, it will track module output mean and std \n",
    "\n",
    "Output: <br> \n",
    "- ForwardHook.means(list): list of means\n",
    "- ForwardHook.stds(list): list of std \n",
    "- ForwardHook.status(str): Hook status (active/removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_stack_activations()`: Stack all output(activations) once all outputs are saved in ForwardHook.activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can be inheritet this class and define your own `hook_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect intermediate activations and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_linear_layer_activations_states(dls, model, layers:list, stats:bool, activation:bool, output:bool, remove_hooks=True):\n",
    "  ''' useful when model is trained and want to analyze intermediate layers'''\n",
    "\n",
    "  hooks = dict()\n",
    "  if output: output_list = []\n",
    "\n",
    "  for layer in layers:\n",
    "    if hasattr(model, layer): hooks[f'{layer}'] = ForwardHook ( getattr(model, layer),layer, activation=activation, stats=stats ) \n",
    "\n",
    "  model.eval()\n",
    "  i = 0\n",
    "  for batch in tqdm(dls, desc='Fatching data: '):\n",
    "    with torch.no_grad():\n",
    "        i+=1\n",
    "        out = model(batch)\n",
    "        if output: output_list.append( (out) ) \n",
    "\n",
    "  for k,v in hooks.items():\n",
    "    if activation: v._stack_activations()\n",
    "    if remove_hooks: v.close()\n",
    "\n",
    "  if output: \n",
    "      return hooks, output_list\n",
    "  else: \n",
    "      return hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "```\n",
    "class BertSentiModel(nn.Module):\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "\n",
    "    super(BertSentiModel, self).__init__()\n",
    "    self.bert_model = transformers.BertModel.from_pretrained(pretrained_model, return_dict=False)\n",
    "    self.kwargs = kwargs\n",
    "    self.lin1 = nn.Linear(768, 10)\n",
    "    self.lin2 = nn.Linear(768, 10)\n",
    "    self.lin3 = nn.Linear(10, 1)\n",
    "    self.lin4 = nn.Linear(10, 1)\n",
    "\n",
    "  def forward(self,batch):\n",
    "    ids, mask, token_type_ids = batch['ids'], batch['mask'], batch['token_type_ids']\n",
    "    _, x = self.bert_model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "    x1 = self.lin3(F.relu(self.lin1(x)))\n",
    "    x2 = self.lin4(F.relu(self.lin2(x)))\n",
    "    return x1, x2\n",
    "    \n",
    "model = BertSentiModel()\n",
    "model.load_state_dict(state_dict)\n",
    "hooks, output_list = get_linear_layer_activations_states(dataloader, model, ['lin1', 'bert_model', 'lin2'], stats=True, activation=True, output=True, remove_hooks=True) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3712jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
